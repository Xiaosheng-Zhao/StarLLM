/home/zxs/.conda/envs/starllm2/lib/python3.10/site-packages/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)
  warnings.warn("xFormers is available (SwiGLU)")
/home/zxs/.conda/envs/starllm2/lib/python3.10/site-packages/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)
  warnings.warn("xFormers is available (Attention)")
/home/zxs/.conda/envs/starllm2/lib/python3.10/site-packages/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)
  warnings.warn("xFormers is available (Block)")
Global seed set to 42
wandb: Currently logged in as: xiaosheng_zhao. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /scratch/zxs/scripts/starllm/back2/AstroCLIP/astroclip/outputs/wandb/run-20240716_153544-th6afglv
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run icy-field-46
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaosheng_zhao/astroclip-spectrum
wandb: üöÄ View run at https://wandb.ai/xiaosheng_zhao/astroclip-spectrum/runs/th6afglv
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [9]

  | Name            | Type       | Params
-----------------------------------------------
0 | data_embed      | Linear     | 17.7 K
1 | position_embed  | Embedding  | 614 K 
2 | dropout         | Dropout    | 0     
3 | blocks          | ModuleList | 42.5 M
4 | final_layernorm | LayerNorm  | 1.5 K 
5 | head            | Linear     | 16.9 K
-----------------------------------------------
43.2 M    Trainable params
0         Non-trainable params
43.2 M    Total params
172.711   Total estimated model params size (MB)
Sanity Checking: 0it [00:00, ?it/s]/home/zxs/.conda/envs/starllm2/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 80 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
Sanity Checking:   0%|          | 0/1 [00:00<?, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]torch.Size([64, 391, 22])
Sanity Checking DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.20it/s]                                                                           /home/zxs/.conda/envs/starllm2/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 80 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/zxs/.conda/envs/starllm2/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (15) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
  rank_zero_warn(
Training: 0it [00:00, ?it/s]Training:   0%|          | 0/16 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/16 [00:00<?, ?it/s] torch.Size([64, 391, 22])
Epoch 0:   6%|‚ñã         | 1/16 [00:06<01:39,  6.64s/it]Epoch 0:   6%|‚ñã         | 1/16 [00:06<01:39,  6.64s/it, loss=2.43, v_num=fglv, training_loss=2.430]torch.Size([64, 391, 22])
Epoch 0:  12%|‚ñà‚ñé        | 2/16 [00:13<01:35,  6.80s/it, loss=2.43, v_num=fglv, training_loss=2.430]Epoch 0:  12%|‚ñà‚ñé        | 2/16 [00:13<01:35,  6.80s/it, loss=2.45, v_num=fglv, training_loss=2.460]torch.Size([64, 391, 22])
Epoch 0:  19%|‚ñà‚ñâ        | 3/16 [00:20<01:27,  6.74s/it, loss=2.45, v_num=fglv, training_loss=2.460]Epoch 0:  19%|‚ñà‚ñâ        | 3/16 [00:20<01:27,  6.74s/it, loss=2.07, v_num=fglv, training_loss=1.320]torch.Size([64, 391, 22])
Epoch 0:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:27<01:21,  6.77s/it, loss=2.07, v_num=fglv, training_loss=1.320]Epoch 0:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:27<01:21,  6.77s/it, loss=nan, v_num=fglv, training_loss=nan.0] torch.Size([64, 391, 22])
Epoch 0:  31%|‚ñà‚ñà‚ñà‚ñè      | 5/16 [00:33<01:14,  6.74s/it, loss=nan, v_num=fglv, training_loss=nan.0]Epoch 0:  31%|‚ñà‚ñà‚ñà‚ñè      | 5/16 [00:33<01:14,  6.74s/it, loss=nan, v_num=fglv, training_loss=nan.0]torch.Size([64, 391, 22])
Epoch 0:  38%|‚ñà‚ñà‚ñà‚ñä      | 6/16 [00:40<01:07,  6.76s/it, loss=nan, v_num=fglv, training_loss=nan.0]Epoch 0:  38%|‚ñà‚ñà‚ñà‚ñä      | 6/16 [00:40<01:07,  6.76s/it, loss=nan, v_num=fglv, training_loss=nan.0]torch.Size([64, 391, 22])
Epoch 0:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 7/16 [00:47<01:00,  6.74s/it, loss=nan, v_num=fglv, training_loss=nan.0]Epoch 0:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 7/16 [00:47<01:00,  6.74s/it, loss=nan, v_num=fglv, training_loss=nan.0]torch.Size([64, 391, 22])
Epoch 0:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:54<00:54,  6.76s/it, loss=nan, v_num=fglv, training_loss=nan.0]Epoch 0:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:54<00:54,  6.76s/it, loss=nan, v_num=fglv, training_loss=nan.0]torch.Size([64, 391, 22])
Epoch 0:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 9/16 [01:00<00:47,  6.74s/it, loss=nan, v_num=fglv, training_loss=nan.0]Epoch 0:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 9/16 [01:00<00:47,  6.74s/it, loss=nan, v_num=fglv, training_loss=nan.0]torch.Size([64, 391, 22])
Epoch 0:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 10/16 [01:07<00:40,  6.75s/it, loss=nan, v_num=fglv, training_loss=nan.0]Epoch 0:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 10/16 [01:07<00:40,  6.75s/it, loss=nan, v_num=fglv, training_loss=nan.0]torch.Size([64, 391, 22])
Epoch 0:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 11/16 [01:14<00:33,  6.73s/it, loss=nan, v_num=fglv, training_loss=nan.0]Epoch 0:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 11/16 [01:14<00:33,  6.73s/it, loss=nan, v_num=fglv, training_loss=nan.0]torch.Size([64, 391, 22])
Epoch 0:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [01:20<00:26,  6.75s/it, loss=nan, v_num=fglv, training_loss=nan.0]Epoch 0:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 12/16 [01:20<00:26,  6.75s/it, loss=nan, v_num=fglv, training_loss=nan.0]torch.Size([64, 391, 22])
Epoch 0:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 13/16 [01:27<00:20,  6.73s/it, loss=nan, v_num=fglv, training_loss=nan.0]Epoch 0:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 13/16 [01:27<00:20,  6.73s/it, loss=nan, v_num=fglv, training_loss=nan.0]torch.Size([64, 391, 22])
Epoch 0:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14/16 [01:34<00:13,  6.74s/it, loss=nan, v_num=fglv, training_loss=nan.0]Epoch 0:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 14/16 [01:34<00:13,  6.74s/it, loss=nan, v_num=fglv, training_loss=nan.0]torch.Size([64, 391, 22])
Epoch 0:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 15/16 [01:40<00:06,  6.73s/it, loss=nan, v_num=fglv, training_loss=nan.0]Epoch 0:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 15/16 [01:40<00:06,  6.73s/it, loss=nan, v_num=fglv, training_loss=nan.0]
Validation: 0it [00:00, ?it/s][A
Validation:   0%|          | 0/1 [00:00<?, ?it/s][A
Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s][Atorch.Size([64, 391, 22])

Validation DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 40.18it/s][AEpoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [01:46<00:00,  6.68s/it, loss=nan, v_num=fglv, training_loss=nan.0]Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [01:47<00:00,  6.69s/it, loss=nan, v_num=fglv, training_loss=nan.0, val_training_loss=nan.0]
                                                                      [AEpoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [01:47<00:00,  6.69s/it, loss=nan, v_num=fglv, training_loss=nan.0, val_training_loss=nan.0]Epoch 0:   0%|          | 0/16 [00:00<?, ?it/s, loss=nan, v_num=fglv, training_loss=nan.0, val_training_loss=nan.0]         Epoch 1:   0%|          | 0/16 [00:00<?, ?it/s, loss=nan, v_num=fglv, training_loss=nan.0, val_training_loss=nan.0]torch.Size([64, 391, 22])
Epoch 1:   6%|‚ñã         | 1/16 [00:06<01:42,  6.81s/it, loss=nan, v_num=fglv, training_loss=nan.0, val_training_loss=nan.0]Epoch 1:   6%|‚ñã         | 1/16 [00:06<01:42,  6.81s/it, loss=nan, v_num=fglv, training_loss=nan.0, val_training_loss=nan.0]torch.Size([64, 391, 22])
Epoch 1:  12%|‚ñà‚ñé        | 2/16 [00:13<01:33,  6.68s/it, loss=nan, v_num=fglv, training_loss=nan.0, val_training_loss=nan.0]Epoch 1:  12%|‚ñà‚ñé        | 2/16 [00:13<01:33,  6.68s/it, loss=nan, v_num=fglv, training_loss=nan.0, val_training_loss=nan.0]torch.Size([64, 391, 22])
Epoch 1:  19%|‚ñà‚ñâ        | 3/16 [00:20<01:27,  6.75s/it, loss=nan, v_num=fglv, training_loss=nan.0, val_training_loss=nan.0]Epoch 1:  19%|‚ñà‚ñâ        | 3/16 [00:20<01:27,  6.75s/it, loss=nan, v_num=fglv, training_loss=nan.0, val_training_loss=nan.0]torch.Size([64, 391, 22])
Epoch 1:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:26<01:20,  6.70s/it, loss=nan, v_num=fglv, training_loss=nan.0, val_training_loss=nan.0]Epoch 1:  25%|‚ñà‚ñà‚ñå       | 4/16 [00:26<01:20,  6.70s/it, loss=nan, v_num=fglv, training_loss=nan.0, val_training_loss=nan.0]torch.Size([64, 391, 22])
Epoch 1:  31%|‚ñà‚ñà‚ñà‚ñè      | 5/16 [00:33<01:14,  6.75s/it, loss=nan, v_num=fglv, training_loss=nan.0, val_training_loss=nan.0]Epoch 1:  31%|‚ñà‚ñà‚ñà‚ñè      | 5/16 [00:33<01:14,  6.75s/it, loss=nan, v_num=fglv, training_loss=nan.0, val_training_loss=nan.0]torch.Size([64, 391, 22])
Epoch 1:  38%|‚ñà‚ñà‚ñà‚ñä      | 6/16 [00:40<01:07,  6.72s/it, loss=nan, v_num=fglv, training_loss=nan.0, val_training_loss=nan.0]Epoch 1:  38%|‚ñà‚ñà‚ñà‚ñä      | 6/16 [00:40<01:07,  6.72s/it, loss=nan, v_num=fglv, training_loss=nan.0, val_training_loss=nan.0]torch.Size([64, 391, 22])
Epoch 1:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 7/16 [00:47<01:00,  6.74s/it, loss=nan, v_num=fglv, training_loss=nan.0, val_training_loss=nan.0]Epoch 1:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 7/16 [00:47<01:00,  6.74s/it, loss=nan, v_num=fglv, training_loss=nan.0, val_training_loss=nan.0]torch.Size([64, 391, 22])
Epoch 1:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:53<00:53,  6.72s/it, loss=nan, v_num=fglv, training_loss=nan.0, val_training_loss=nan.0]Epoch 1:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 8/16 [00:53<00:53,  6.72s/it, loss=nan, v_num=fglv, training_loss=nan.0, val_training_loss=nan.0]