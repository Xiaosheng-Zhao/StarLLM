/home/zxs/.conda/envs/starllm2/lib/python3.10/site-packages/dinov2/layers/swiglu_ffn.py:43: UserWarning: xFormers is available (SwiGLU)
  warnings.warn("xFormers is available (SwiGLU)")
/home/zxs/.conda/envs/starllm2/lib/python3.10/site-packages/dinov2/layers/attention.py:27: UserWarning: xFormers is available (Attention)
  warnings.warn("xFormers is available (Attention)")
/home/zxs/.conda/envs/starllm2/lib/python3.10/site-packages/dinov2/layers/block.py:33: UserWarning: xFormers is available (Block)
  warnings.warn("xFormers is available (Block)")
Global seed set to 42
wandb: Currently logged in as: xiaosheng_zhao. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.4
wandb: Run data is saved locally in /scratch/zxs/scripts/starllm/back2/AstroCLIP/astroclip/outputs/wandb/run-20240716_150145-bhtlgfo3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run soft-sky-11
wandb: ‚≠êÔ∏è View project at https://wandb.ai/xiaosheng_zhao/astroclip-alignment
wandb: üöÄ View run at https://wandb.ai/xiaosheng_zhao/astroclip-alignment/runs/bhtlgfo3
/home/zxs/.conda/envs/starllm2/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:263: UserWarning: Attribute 'image_encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['image_encoder'])`.
  rank_zero_warn(
/home/zxs/.conda/envs/starllm2/lib/python3.10/site-packages/lightning/pytorch/utilities/parsing.py:263: UserWarning: Attribute 'spectrum_encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['spectrum_encoder'])`.
  rank_zero_warn(
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [9]

  | Name             | Type         | Params
--------------------------------------------------
0 | image_encoder    | SpectrumHead | 55.2 M
1 | spectrum_encoder | SpectrumHead | 55.2 M
2 | criterion        | CLIPLoss     | 0     
--------------------------------------------------
24.1 M    Trainable params
86.4 M    Non-trainable params
110 M     Total params
441.989   Total estimated model params size (MB)
Sanity Checking: 0it [00:00, ?it/s]/home/zxs/.conda/envs/starllm2/lib/python3.10/site-packages/lightning/pytorch/utilities/data.py:110: UserWarning: Total length of `DataLoader` across ranks is zero. Please make sure this was your intention.
  rank_zero_warn(
                                   /home/zxs/.conda/envs/starllm2/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.
  rank_zero_warn(
Training: 0it [00:00, ?it/s]Training:   0%|          | 0/3 [00:00<?, ?it/s]Epoch 0:   0%|          | 0/3 [00:00<?, ?it/s] Epoch 0:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:49<01:39, 49.52s/it]Epoch 0:  33%|‚ñà‚ñà‚ñà‚ñé      | 1/3 [00:49<01:39, 49.52s/it, loss=5.56, v_num=gfo3]Epoch 0:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:51<00:25, 25.63s/it, loss=5.56, v_num=gfo3]Epoch 0:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 2/3 [00:51<00:25, 25.64s/it, loss=5.56, v_num=gfo3]Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:53<00:00, 17.67s/it, loss=5.56, v_num=gfo3]Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:53<00:00, 17.67s/it, loss=5.56, v_num=gfo3]Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:53<00:00, 17.67s/it, loss=5.56, v_num=gfo3]/home/zxs/.conda/envs/starllm2/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:360: UserWarning: `ModelCheckpoint(monitor='val_loss_nologit')` could not find the monitored key in the returned metrics: ['train_loss_withlogit', 'train_loss_nologit', 'scale', 'epoch', 'step']. HINT: Did you call `log('val_loss_nologit', value)` in the `LightningModule`?
  warning_cache.warn(m)
`Trainer.fit` stopped: `max_epochs=1` reached.
Epoch 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:54<00:00, 18.12s/it, loss=5.56, v_num=gfo3]
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.010 MB uploadedwandb: / 0.010 MB of 0.010 MB uploadedwandb: - 0.010 MB of 0.011 MB uploadedwandb: \ 0.010 MB of 0.017 MB uploadedwandb: | 0.017 MB of 0.017 MB uploadedwandb: üöÄ View run soft-sky-11 at: https://wandb.ai/xiaosheng_zhao/astroclip-alignment/runs/bhtlgfo3
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/xiaosheng_zhao/astroclip-alignment
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./astroclip/outputs/wandb/run-20240716_150145-bhtlgfo3/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
